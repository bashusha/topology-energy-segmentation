\section{Energy-based Segmentation Algorithm}

This section describes the concrete algorithmic realization of the proposed
diagnostic framework. The goal is not to optimize predictive performance
directly, but to explore the space of possible segmentations and assess whether
any of them provide stable and interpretable reductions in structural
inconsistency between covariates and targets.

The algorithm operates on a graph-based representation of the data and produces
a sequence of candidate segmentations with increasing structural complexity.
Rather than selecting a single partition a priori, it exposes the full
energy--complexity trade-off for diagnostic inspection.

\subsection{Notation and energy definition}

Let $G = (V,E)$ denote a locality graph constructed over a set of $n$ landmark
observations. Each node $i \in V$ corresponds to a landmark point $(X_i, F_i)$.
A segmentation is represented by zone labels
\[
z_i \in \{1,\dots,K\}, \quad i \in V,
\]
where $K$ denotes the number of zones.

Each zone $k$ is associated with parameters $\phi_k$ summarizing the local
behavior of the target variable $F \mid X$ within that zone. The specific form
of $\phi_k$ depends on the application and may range from simple moment-based
summaries to fitted local predictive or distributional models.

We define an energy functional of the form
\[
E(z,\phi)
\;=\;
\sum_{i=1}^{n} \ell_i\bigl(\phi_{z_i}\bigr)
\;+\;
\lambda \sum_{(i,j)\in E} w_{ij}\,\mathbf{1}[z_i \neq z_j]
\;+\;
\alpha\,K .
\]

The first term is a \emph{data-consistency term} measuring how well observation
$i$ agrees with the target behavior of its assigned zone.
The second term is a \emph{boundary penalty} that discourages separating
neighboring observations with similar target behavior; edge weights $w_{ij}$
encode local disagreement in $F \mid X$.
The final term penalizes the number of zones and controls structural complexity.

From a frequentist perspective, this energy corresponds to a regularized
objective balancing explanatory adequacy against model complexity.
From a Bayesian perspective, the same functional can be interpreted as a
negative log-posterior, with the complexity term acting as an implicit prior
favoring parsimonious structural explanations. Importantly, both views lead to
the same computational procedure.

\subsection{Edge weights and disagreement metrics}

The quality of the diagnostic segmentation depends critically on how local
disagreement in target behavior is quantified. Direct computation of rich
distributional discrepancies for all candidate segmentations is often
computationally infeasible.

To address this, we adopt a two-phase strategy.

In \emph{Phase A}, surrogate disagreement measures $\tilde{w}_{ij}$ are computed
for each edge $(i,j)\in E$ using low-complexity summaries of local target
behavior, such as moment differences, residual contrasts, or simplified
predictive discrepancies. These surrogates are designed to be inexpensive and
robust, and are used solely to guide the exploration of candidate partitions.

In \emph{Phase B}, for a restricted set of candidate segmentations, edge weights
$w_{ij}$ are recomputed using richer discrepancy measures, such as energy
distance, Hellinger distance between fitted mixtures, or posterior predictive
divergence in Bayesian models. This refinement step is used for validation and
interpretation rather than for large-scale search.

\subsection{Two-phase optimization procedure}

The algorithm follows an agglomerative strategy operating on the locality graph.
Starting from singleton zones, zones are iteratively merged to minimize a
surrogate energy
\[
\tilde{E}(z)
=
\sum_{(i,j)\in E} \tilde{w}_{ij}\,\mathbf{1}[z_i \neq z_j]
+
\alpha\,K .
\]

This procedure produces an ordered sequence of candidate partitions, along with
the full energy path as a function of $K$. The role of surrogate metrics at this
stage is intentionally limited: false positives are acceptable and filtered
out during refinement, while false negatives are mitigated by conservative
thresholding and neighborhood aggregation.

Rather than selecting a single optimum automatically, the algorithm retains
multiple candidate segmentations corresponding to stable regions or elbows in
the energy trajectory. For these candidates, the refined energy $E(z)$ is
evaluated using richer disagreement measures.

If no candidate segmentation yields a meaningful or stable improvement over the
global model ($K=1$), the algorithm explicitly returns the unsegmented solution,
treating abstention from segmentation as a valid diagnostic outcome.

\subsection{Temporal consistency}

When data are available across multiple time slices, the diagnostic procedure
can be applied independently to each slice. Resulting segmentations are
compared using overlap measures such as the adjusted Rand index.

Zones that persist across time are interpreted as stable structural obstacles to
a single global model. In contrast, zones that appear only sporadically under
increased noise or scale changes are treated as artifacts rather than actionable
signals. Temporal consistency thus provides an additional layer of validation
for segmentation decisions.

\subsection{Algorithmic summary}

Algorithm~\ref{alg:topology_energy_segmentation} summarizes the full
topology-aware energy-based segmentation procedure, including landmark
selection, graph construction, surrogate screening, refinement, and diagnostic
output.
